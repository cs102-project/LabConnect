% Use either your LaTeX editor or latexmk to compile.

\documentclass[a4paper, 12pt]{article}

% Document quality things
\usepackage[utf8]{inputenc}
\usepackage{microtype, xcolor}
\usepackage{url, hyperref}
\hypersetup{colorlinks=true, linkcolor=blue, citecolor=black, urlcolor=blue}

% Setting margins
\usepackage[a4paper, left=2cm, right=2cm, top=1.75cm, bottom=1.75cm, includefoot]{geometry}

% Table helper packages
\usepackage{multirow, multicol}
\usepackage{makecell}
\usepackage{array}
%\usepackage{tabularx} % Not needed currently, but has a few nice options
%\usepackage{wrapfig} % Floating figures/tables

% Prevents spamming tedious newlines everywhere, also disables auto indentation, etc.
\usepackage[skip=0.75\baselineskip plus 2pt]{parskip}

% Self-explanatory
\usepackage{titlesec}
\titleformat{\section}[block]{\normalfont\scshape\Large}{\thesection}{1em}{}
\titleformat{\subsection}{\normalfont\large}{\thesubsection}{1em}{}

% Referencing
\usepackage[backend=bibtex, style=numeric-comp, sorting=none]{biblatex}
\addbibresource{bibliography.bib}

\begin{document}

    % Header Table
    \begin{table}[h!]
        \renewcommand{\arraystretch}{3}
        \centering
        \begin{tabular}{ | >{\raggedleft\arraybackslash}m{3cm} l >{\raggedleft\arraybackslash}m{3cm} m{3cm} | }
            \hline
            \Huge CS 102 & \textit{Spring 2020/21} & \multirow{2}{*}{\makecell{Project\\Group}} & \multirow{2}{*}{\textbf{\Huge G2C}} \\
            \makecell[r]{Instructor:\\Assistant:} & \makecell[l]{\textbf{Aynur Dayanık}\\\textbf{Haya Shamim Khan Khattak}} & & \\
            \hline
        \end{tabular}
    \end{table}

    % Grading Table
    \begin{table}[h!]
            \renewcommand{\arraystretch}{1.4}
            \centering
            \footnotesize
            \begin{tabular}{ l p{1.5cm} | p{1.5cm} | }
                \hline
                \multicolumn{1}{|c|}{\textbf{Criteria}} & \multicolumn{1}{c|}{\textbf{TA/Grader}} & \multicolumn{1}{c|}{\textbf{Instructor}} \\ \hline
                \multicolumn{1}{|p{10.5cm}|}{Presentation} &  &  \\[10ex] \hline
                \multicolumn{1}{r|}{\textbf{Overall}} &  &  \\
                \cline{2-3}
            \end{tabular}
    \end{table}

    % Project Information Header
    {\centering\Huge \bfseries \raisebox{0.5ex}{\texttildelow} LabConnect \raisebox{0.5ex}{\texttildelow} \par}

    %{\centering\large Group Name \par}

    \begin{table}[h!]
        \renewcommand{\arraystretch}{1.4}
        \centering
        \small
        \begin{tabular}{ r l }
            \textbf{Borga Haktan Bilen} & 22002733 \\
            \textbf{Vedat Eren Arıcan} & 22002643 \\
            \textbf{Berkan Şahin} & 22003211 \\
            \textbf{Berk Çakar} & 22003021 \\
            \textbf{Alp Ertan} & 22003912 \\
        \end{tabular}
    \end{table}

    % Document Type Header Table
    \begin{table}[h!]
        \renewcommand{\arraystretch}{1.5}
        \centering
        \begin{tabular}{ |>{\centering\arraybackslash}m{15.15cm}| }
            \hline
            \Large \textbf{Requirements Report} \\
            \small (version 3.0) \\
            \small \textbf{\today} \\
            \hline
        \end{tabular}
    \end{table}

    % Document begins here...

    \section{Introduction}

    LabConnect facilitates communication between students, TA's, tutors,
    and instructors. In the background, it is mainly a web application
    (If sensible/necessary, it may possibly be ported to Android) that aims 
    to assist CS introductory courses in terms of organization and communication. 
    Proposed ideas for features include priority queuing for TA zoom rooms among many other 
    enhancements to TA/instructor productivity. For example, those who have completed their labs 
    can be tested using pre-defined (by TA or instructor) unit tests, if students pass the 
    tests successfully then they will be ordered by the number of visits to TA 
    in the same session, in order to decrease waiting times for the students 
    who are waiting from the beginning, and to optimize the process in general. 
    TA's can also use the system to see previous versions of each student's code 
    in a more practical way, similar to real version control managers in spirit. 
    The style guidelines put forth by the instructors can be enforced automatically by parsing
    the student's sent code files. Much of the repetitive work that course
    staff need to do can be reduced substantially by automated actions,
    allowing TA's to allocate time for more hands-on help towards students.
    The student experience can be improved further by adding helpful
    features such as personal notes for students and so on.

    \subsection{Related Work}

    As a part of their curriculum, most of the big engineering institutions and universities have hands-on laboratory sessions, that are mandatory
    for students to attend them. At some particularly crowded institutions; for each lab session, every student has to submit approximately 9 to 12 assignments. 
    Additionally, students and the teaching staff alike may also need to deal with lab tests, perhaps even multiple of them. 
    Cumulatively, the amount of submissions can reach approximately 10000 per semester. 
    Even if there were 20 evaluators, each evaluator would need to take care of almost 500 assignments.
    Without the help of automation, evaluators spend most of their time grading and testing work, rather than creating more useful assignments for students 
    or simply spending more time assisting them \cite{Mandal2007}.

    Some of the positive sides of the automated grading are syntactic correctness, maintainability and efficiency.
    Moreover, automated grading systems weigh down the lack of objectivity in conventional grading and these systems
    can track all of the building process. In the contemporary context, these kinds of automated grading systems are commonly used by
    some commercial competitive programming and recruitment sites (for instance, HackerRank, TopCoder and HackerEarth) \cite{RestrepoCalle2018}.

    At the same time, the quantity of the feedback given by the teaching staff is directly impactful to the effectiveness of the student's work that is done
    after the class. The students have an easier time studying weak areas if they are informed about their
    performance properly. The autonomous nature of instructor feedback frees everyone from manual assessment,
    which is a time consuming and unproductive task \cite{RestrepoCalle2018-2}.

    If there are many more students than there are teachers, this can hinder the process of manual assessment of the student's code.
    Although this problem can be solved by increasing the amount of teachers, this solution can harm the university economically.
    The idea of automated assessment comes into play with its ability to solve this problem without increased costs.
    There is much evidence to assert that student code is evaluated more properly using automatic assessment rather than using pen\&paper testing.
    Research also interestingly shows that automated assessment reduces the rate of dropout in programming majors \cite{Gordillo2019}.

    According to one source, the effectiveness of different testing procedures are as follows:
    Unit testing is able to catch 50\% of the errors.
    Integration testing is able to catch 40\% of the errors.
    Regression testing is able to catch 30\% of the errrors \cite{Fenton2018}.

    The evaluation system decreases the work load of the evaluation process since it makes the evalution process easier.
    Just as the system has automatic testing, it also comes with automatic grading. Additionally, it manages students by aiding
    grouping and scheduling issues, helping with the students checking in to lab sessions, the deadlines and overall project grades.
    As the system is ideally available at all times, the students will be able to get feedback instantly on the correctness of
    their solutions. This way, students are able to keep working on their work at their own pace \cite{Nogueira2011}.

    And finally, it was seen that the approach used for automation of assignments in education is used in DevOps fields in a similar way \cite{Faber2020}.

    \section{Details}

    LabConnect is designed to contain three user interfaces for instructors, students,
    and assistants/graders. It will also contain a server-side program where the submissions
    are stored and tested. This section will provide details on proposed features of the software at the time of this writing.

    \subsection{LabConnect - Instructor Side}

    \subsubsection{Task Definition Stage}

    \begin{itemize}
        \item The instructor is able to input the name and the programming language of the assignment.
        \item The instructor can upload the instructions either as a document, or as a Markdown
          or a plaintext file, after which, it will be rendered and displayed on the website.
        \item The instructor can configure the unit tests as input-output pairs and group them if
          they wish. Some groups of unit tests can be hidden, in which case they won't be
          shown to the students prior to submission.
        \item The instructor can provide a tester class that tests the code provided by the student
        by calling the methods. This class is the one that will be run by the server. This allows students
        to be more flexible in their own main class.
        \item The instructor can determine a time constraint for unit tests. If the execution
          of the code takes longer than the determined time, it will fail said test.
        \item The instructor can determine a time frame for submissions. They can determine a
          seperate deadline for re-submissions, if they wish.
        \item The instructor can assign students to assistants either at random or by hand.
          They can also choose to not assign assistants at all, in which case the students
          will be assigned to the assistants during the lab session, based on the length
          of the queue.
        \item The instructor can either define PAIN-style (Proficient, Acceptable, Incomplete, Nothing)
          tiers for grading or can define certain criteria for which assistants can enter a numeric value. If they
          decide to use tiers, they can define certain tiers (such as Proficient and Acceptable) as ``complete'',
          in which case the submission of a student receiving said grades will automatically be classified as complete.
          If they decide to use numeric grades, they can define a certain threshold that needs to be exceeeded for a
          submission to be classified as complete.
    \end{itemize}

    \subsubsection{Task Update Stage}

    \begin{itemize}
        \item The instructor should ideally have better control over how students' codes are tested.
        The instructor is also able to add more unit tests as the lab progresses.
        The students' unit tests can be updated within the lab period, so any mistakes made
        on the tests themselves can be corrected this way.
    \end{itemize}

    \subsubsection{After the Lab}

    \begin{itemize}
        \item The common errors that are made by students such as missing documentation (i.e. JavaDoc)
        of the written code, and predefined conventions that aren't followed (naming conventions, styling
        guidelines, etc.), will be detected by LabConnect. This data will be shared with the student
        and their instructor. The instructor can later on determine to act up on the most
        important mistakes that are made by the students.
        \item Instructors are able to see which unit test cases of the program
        the students mostly fail at. These unit tests can again reveal the common weaknesses
        of programmers.
        \item The instructor is able to assess their students properly by considering their
        performance in the lab, for which, LabConnect will supply additional analytics. 
        As in, LabConnect will provide detailed performance of the student, based on the mistakes they made, and overall unit test scores. 
        This information can help the instructor achieve a better grasp of their students since the data is properly organised
        and accessible.
    \end{itemize}

    \subsection{LabConnect - Student Side}

    \subsubsection{Code Submission Process}

    \begin{itemize}
      \item The students' files can be individually uploaded as specified by the instructor during the task specification stage.
      As such, they won't have to reupload their whole assignment each time a certain file needs to be modified.
      \item Automated tests and checks are run on the server side each time the student submits their assignment.
      \item Submissions that fail any test are marked as incomplete and the student is redirected to the revision stage.
      Otherwise, the student is placed into the code review queue.
    \end{itemize}

    \subsubsection{Code Review Queue}

    \begin{itemize}
      \item The prerequisite of the code review queue is to pass all (or possibly most) unit tests.
      \item Students waiting in line are shown how many students are waiting in front of them. Student places in the queue are
      stored on the server side, so in the event of a momentary disconnection, places in the queue are preserved.
      \item Students at the top of the queue are sent a prompt to confirm that they are ready to be graded by the TA, after which they will be sent a meeting link. 
      In the event that they fail to confirm in a certain amount of time, they will lose their place in the queue and will be sent to the end of the current queue.
    \end{itemize}

    \subsubsection{Code Revision Process}

    \begin{itemize}
      \item Students whose submission is marked as incomplete either by the TA or the system will be redirected to the revision stage.
      \item The student can either receive written feedback from their TA or a message generated server-side explaining roughly
      what went wrong during the tests. In case of a runtime exception/compiler error, the stack trace is also provided and the lines causing the
      problem are highlighted. Code can also be highlighted manually by the TA, as described in the Code Review Process section of the TA interface.
      \item Students are able to resubmit their files once they are confident that they have solved the issues in the given feedback.
    \end{itemize}

    \subsection{LabConnect - Grader/Assistant Side}

    \subsubsection{Code Review Process}

    \begin{itemize}
        \item At the start of the lab session, the assistants enter their meeting links to the system.
        This allows for the links to be distributed to the students when it is their turn.
        \item The assistant will be directed to the code review interface which contains information about the student, 
        as well as the source code submitted by the student and a field to write feedback about the submission.
        \item The assistant/grader can choose to browse the code submitted by the student from their computer without wasting bandwidth
        and time with screen sharing. They can also write feedback referencing specific lines in the code to make their
        feedback clearer.
        \item After evaluating the code, the assistants can either give the assignment a score or pick
          from the tiers determined by the instructor. If the determined grade or tier is within the satisfactory
          threshold, the student's assignment is marked as complete. Otherwise, the assignment is returned to the
          student with the feedback written by the TA.
        \item The assistant can manually confirm that they are ready for the link to be revealed to the next student in queue. 
        Hence, allowing the assistants to take short breaks if necessary.
    \end{itemize}

    \subsubsection{Task Revision Process}

    \begin{itemize}
      \item The program is able to detect common errors made in the student's code.
      Furthermore, the unit tests can be arranged in a way such that they test specific skills that are
      required. These enhancements to the revision process are aimed to increase the efficiency of the
      graders during live lab sessions.
      \item The Grader can create announcements for the students if there is a highly repeated
      mistake. This way, they won't need to repeat the same small correction for each student,
      and they will find it easier to correct other unique mistakes that the students
      have made. The graders may find greater motivation on teaching the students proper techniques
      if they are freed from the repetitiveness of correcting the same mistakes. Of course, the student can demand
      further help for their "repeated" mistakes.
    \end{itemize}

    \subsection{LabConnect - Server Side}

    \subsubsection{Task Definition Stage}
    \begin{itemize}
      \item The distribution of students to the TAs, whose registration should be done by instructor (probably once at the
      beginning of the semester), are stored in the database.
      \item For each TA, a unique identification number is generated and the students who are allocated to a
      TA will get the identification number of that TA.
      \item For each lab, every student will have their own git repository (created automatically by the system) for the storage of the
      code on the server-side.
      \item The test cases (inputs \& outputs), that are uploaded by instructor, will be stored in the database.
    \end{itemize}

    \subsubsection{Post-Submission Stage}

    \begin{itemize}
      \item Each submission is stored in a version control system. This allows LabConnect to provide easy access to
      submission history for TAs and instructors.
      \item If the current submission doesn't differ from the last submission, it is automatically rejected.
      \item When a student makes a submission, the server-side program then compiles the source code in the submission
      files, if the language picked by the instructor is a compiled language.
      \item Then the server-side program runs either the resulting binary if the language is a compiled language, or the
      interpreter for the language if it is an interpreted one.
      \item If there's an error during the compilation or interpretation/execution stages, the submission is automatically failed and
      the error message is shown to the student.
      \item Using I/O redirection, the input parts of the unit tests are fed to the standard input of the tester program and the output is captured.
      This allows for easy multi-language support and doesn't require students to deal with file I/O while writing the program.
      \item If the output of the program doesn't match the expected results, the submission is automatically failed and the input, expected
      output and actual output for the specific unit test is shown to the student.
      \item The submission is run separately for each unit test. These runs are timed and the results for each unit test is stored in a database.
      If the instructor has specified a time limit for tests, the test in question will automatically fail once the time limit is reached.
    \end{itemize}

    \subsubsection{Queue Management}
    \begin{itemize}
      \item Once a submission passes all the automated tests, the student who made the submission is added to the queue for their TA.
      \item The students that haven't yet received any manual code review are prioritized over those that have received one. This allows each student to
      get their code reviewed at least once during the labs.
      \item If the TA's were not assigned to students by the instructor, the server will pick a TA on the spot based on the queue length. This
      behavior will help make the workloads of the TA's more balanced.
    \end{itemize}

    \subsubsection{Statistics Generation}

    \begin{itemize}
      \item After the lab is finished, the data acquired from the unit tests, the error detection algorithm, etc. is used
      to create statistics about the students' performance. The server can maintain this data for further use.
    \end{itemize}

    \section{Summary \& Conclusions}

    The main purpose of LabConnect is to provide students and instructors a place where everyone will have an easier time
    collaborating with each other properly. Especially in online education, instructors can have a hard time monitoring students'
    performance on labs. With LabConnect, instructors can shape and take control of their classes according to the response given to the lab sessions.
    
    There are some types of software (for instance, Mooshak, Ejudge) that are similar to what LabConnect aims to achieve, however, they are not directly 
    developed to supply instructors/TAs/students with quality of life enhancements in terms of CS courses and their lab sessions/assignments. 
    These kinds of software usually exist for the sake of judging programming contests automatically, which is a vital task for contests
    with high participation, similar to the vitality of automating CS course evaluations with a high participant count. 
    Other related systems include: Code Wars, Hackerrank, Leetcode, and so on.
    These systems also are not intended to be used for interactive education, but rather for self-education.
    However, the popularity and success of these self-education systems nonetheless suggests a viability for the type of 
    automation that LabConnect is attempting to undertake.
    
    A CS course that successfully integrates LabConnect to their education process may enjoy the benefit of automation over
    numerous aspects of the course that are often inefficiently evaluated by the teaching staff manually. Students may also have an easier time
    as features such as the queue system are designed to improve their communication process with the teaching staff. 
    The downside of this integration may be that the flexibility of an entirely manual evaluation process will inevitably be higher compared to
    the flexibility offered by LabConnect. This downside, however, is not as problematic as it may seem. The teaching staff may choose to
    include only certain parts of the LabConnect system, continuing to perform the remaining parts manually as per usual.
    The development of LabConnect aims to provide such modularity to balance the compromise of flexibility as much as possible.

    \pagebreak

    \printbibliography
\end{document}
